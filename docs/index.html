<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice AI Assistant</title>
    <!-- Tailwind CSS CDN for styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Custom font for a modern look */
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f0f4f8; /* Light background */
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            padding: 20px;
            box-sizing: border-box;
        }
        .container {
            background-color: #ffffff;
            border-radius: 1.5rem; /* More rounded corners */
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.1); /* Softer shadow */
            padding: 2.5rem; /* More padding */
            max-width: 600px;
            width: 100%;
            text-align: center;
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }
        .text-area {
            min-height: 100px;
            background-color: #f8fafc; /* Lighter input background */
            border: 1px solid #e2e8f0;
            border-radius: 0.75rem; /* Rounded text areas */
            padding: 1rem;
            text-align: left;
            word-wrap: break-word; /* Ensure text wraps */
            overflow-y: auto; /* Scroll if content overflows */
            font-size: 1rem;
            color: #334155;
            white-space: pre-wrap; /* Preserve whitespace for better display of long text */
        }
        .button-primary {
            background-image: linear-gradient(to right, #6366f1, #8b5cf6); /* Gradient button */
            color: white;
            padding: 0.85rem 1.5rem;
            border-radius: 0.75rem;
            font-weight: 600;
            transition: all 0.2s ease-in-out;
            box-shadow: 0 4px 10px rgba(99, 102, 241, 0.3);
            cursor: pointer;
            border: none;
            outline: none;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 0.5rem;
        }
        .button-primary:hover {
            box-shadow: 0 6px 15px rgba(99, 102, 241, 0.4);
            transform: translateY(-2px);
        }
        .button-primary:active {
            transform: translateY(0);
            box-shadow: 2px 5px rgba(0, 0, 0, 0.2);
        }
        .button-primary:disabled {
            background-image: none;
            background-color: #cbd5e1; /* Gray out when disabled */
            cursor: not-allowed;
            box-shadow: none;
            transform: none;
        }
        .loading-spinner {
            border: 4px solid rgba(0, 0, 0, 0.1);
            border-left-color: #6366f1;
            border-radius: 50%;
            width: 24px;
            height: 24px;
            animation: spin 1s linear infinite;
            display: inline-block;
            vertical-align: middle;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        .message-box {
            border-radius: 0.75rem;
            padding: 1rem;
            margin-top: 1rem;
            text-align: left;
            font-size: 0.9rem;
            display: none; /* Hidden by default */
        }
        .message-box.show {
            display: block;
        }
        .message-box.error {
            background-color: #ffe4e6; /* Light red for errors */
            color: #dc2626; /* Darker red text */
            border: 1px solid #fecaca;
        }
        .message-box.info {
            background-color: #d1fae5; /* Light green for info */
            color: #065f46; /* Darker green text */
            border: 1px solid #a7f3d0;
        }
        .select-container {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
            text-align: left;
        }
        .select-container select {
            padding: 0.75rem;
            border: 1px solid #e2e8f0;
            border-radius: 0.75rem;
            background-color: #f8fafc;
            font-size: 1rem;
            color: #334155;
            appearance: none; /* Remove default arrow */
            background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 20 20' fill='currentColor'%3E%3Cpath fill-rule='evenodd' d='M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z' clip-rule='evenodd'/%3E%3C/svg%3E");
            background-repeat: no-repeat;
            background-position: right 0.75rem center;
            background-size: 1.5em 1.5em;
        }
        @keyframes pulse {
            0% { transform: scale(1); box-shadow: 0 0 0 0 rgba(99, 102, 241, 0.4); }
            70% { transform: scale(1.05); box-shadow: 0 0 0 15px rgba(99, 102, 241, 0); }
            100% { transform: scale(1); box-shadow: 0 0 0 0 rgba(99, 102, 241, 0); }
        }
        .hotword-listening {
            animation: pulse 2s infinite;
        }
        .speaking-active {
            color: #10b981; /* Green for speaking */
            font-weight: 700;
        }
        .recording-active {
            color: #ef4444; /* Red color for recording */
            font-weight: 700;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Voice Assistant App Section -->
        <div id="app-section">
            <h1 class="text-3xl font-bold text-gray-800"> AI Assistant RAJI TECHNOLGY +251913395143</h1>

            <div class="select-container">
                <label for="language-select" class="text-gray-700 font-medium">Select Input Language:</label>
                <select id="language-select" class="w-full">
                    <option value="en-US">English (US)</option>
                    <option value="es-ES">Spanish (Spain)</option>
                    <option value="fr-FR">French (France)</option>
                    <option value="de-DE">German (Germany)</option>
                    <option value="it-IT">Italian (Italy)</option>
                    <option value="pt-BR">Portuguese (Brazil)</option>
                    <option value="zh-CN">Chinese (Mandarin, Simplified)</option>
                    <option value="ja-JP">Japanese (Japan)</option>
                    <option value="ko-KR">Korean (South Korea)</option>
                    <option value="ar-SA">Arabic (Saudi Arabia)</option>
                    <option value="ru-RU">Russian (Russia)</option>
                    <option value="hi-IN">Hindi (India)</option>
                    <option value="am-ET">Amharic (Ethiopia)</option>
                </select>
            </div>

            <div class="select-container">
                <label for="voice-select" class="text-gray-700 font-medium">Select Speaker Voice:</label>
                <select id="voice-select" class="w-full"></select>
            </div>
            
            <!-- Status message area, replacing the button -->
            <div id="status-message" class="text-center text-gray-600 font-medium flex items-center justify-center gap-2">
                <span>Loading...</span>
                <div id="loading-spinner" class="loading-spinner"></div>
            </div>

            <div class="flex flex-col gap-2">
                <label for="user-input" class="text-left text-gray-700 font-medium">Your Input:</label>
                <div id="user-input" class="text-area" aria-live="polite"></div>
            </div>

            <div class="flex flex-col gap-2">
                <label for="assistant-output" class="text-left text-gray-700 font-medium">Assistant's Response:</label>
                <div id="assistant-output" class="text-area" aria-live="polite"></div>
            </div>

        </div>
        <div id="message-box" class="message-box" aria-live="assertive"></div>
    </div>

    <script type="module">
        // Get references to DOM elements
        const statusMessageDiv = document.getElementById('status-message');
        const loadingSpinner = document.getElementById('loading-spinner');
        const userInputDiv = document.getElementById('user-input');
        const assistantOutputDiv = document.getElementById('assistant-output');
        const messageBox = document.getElementById('message-box');
        const languageSelect = document.getElementById('language-select');
        const voiceSelect = document.getElementById('voice-select');

        // Check for Web Speech API support
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
            showMessage("Your browser does not support Speech Recognition. Please use a modern browser like Chrome or Edge.", true);
            statusMessageDiv.textContent = 'Browser Not Supported';
        }

        let recognition; // The main recognition instance for user queries
        let hotwordRecognition; // The separate instance for detecting the hotword
        let currentAudio; // Reference to the currently playing audio element
        
        let state = 'LOADING'; // 'LOADING', 'HOTWORD_LISTENING', 'MAIN_LISTENING', 'THINKING', 'SPEAKING'
        
        // Prebuilt voices for gemini-2.5-flash-preview-tts
        const prebuiltVoices = [
            { name: "Kore", label: "Kore (Firm)" },
            { name: "Puck", label: "Puck (Upbeat)" },
            { name: "Zephyr", label: "Zephyr (Bright)" },
            { name: "Charon", label: "Charon (Informative)" },
            { name: "Fenrir", label: "Fenrir (Excitable)" },
            { name: "Leda", label: "Leda (Youthful)" },
            { name: "Orus", label: "Orus (Firm)" },
            { name: "Aoede", label: "Aoede (Breezy)" },
            { name: "Callirrhoe", label: "Callirrhoe (Easy-going)" },
            { name: "Autonoe", label: "Autonoe (Bright)" },
            { name: "Enceladus", label: "Enceladus (Breathy)" },
            { name: "Iapetus", label: "Iapetus (Clear)" },
            { name: "Umbriel", label: "Umbriel (Easy-going)" },
            { name: "Algieba", label: "Algieba (Smooth)" },
            { name: "Despina", label: "Despina (Smooth)" },
            { name: "Erinome", label: "Erinome (Clear)" },
            { name: "Algenib", label: "Algenib (Gravelly)" },
            { name: "Rasalgethi", label: "Rasalgethi (Informative)" },
            { name: "Laomedeia", label: "Laomedeia (Upbeat)" },
            { name: "Achernar", label: "Achernar (Soft)" },
            { name: "Alnilam", label: "Alnilam (Firm)" },
            { name: "Schedar", label: "Schedar (Even)" },
            { name: "Gacrux", label: "Gacrux (Mature)" },
            { name: "Pulcherrima", label: "Pulcherrima (Forward)" },
            { name: "Achird", label: "Achird (Friendly)" },
            { name: "Zubenelgenubi", label: "Zubenelgenubi (Casual)" },
            { name: "Vindemiatrix", label: "Vindemiatrix (Gentle)"},
            { name: "Sadachbia", label: "Sadachbia (Lively)"},
            { name: "Sadaltager", label: "Sadaltager (Knowledgeable)"},
            { name: "Sulafat", label: "Sulafat (Warm)"}
        ];
        
        /**
         * Updates the UI based on the current state.
         */
        function updateUI() {
            statusMessageDiv.classList.remove('hotword-listening', 'recording-active', 'speaking-active');
            loadingSpinner.classList.add('hidden');
            languageSelect.disabled = false;
            voiceSelect.disabled = false;

            switch(state) {
                case 'LOADING':
                    statusMessageDiv.textContent = 'Loading...';
                    loadingSpinner.classList.remove('hidden');
                    languageSelect.disabled = true;
                    voiceSelect.disabled = true;
                    break;
                case 'HOTWORD_LISTENING':
                    statusMessageDiv.textContent = 'Listening for "Jarvis"...';
                    statusMessageDiv.classList.add('hotword-listening');
                    break;
                case 'MAIN_LISTENING':
                    statusMessageDiv.textContent = 'Listening...';
                    statusMessageDiv.classList.add('recording-active');
                    break;
                case 'THINKING':
                    statusMessageDiv.textContent = 'Thinking...';
                    loadingSpinner.classList.remove('hidden');
                    languageSelect.disabled = true;
                    voiceSelect.disabled = true;
                    break;
                case 'SPEAKING':
                    statusMessageDiv.textContent = "Assistant Speaking...";
                    statusMessageDiv.classList.add('speaking-active');
                    break;
            }
        }

        /**
         * Sets the application state and updates the UI.
         * @param {string} newState - The new state to set.
         */
        function setState(newState) {
            state = newState;
            updateUI();
        }

        /**
         * Displays a message to the user in the message box.
         * @param {string} message - The message to display.
         * @param {boolean} isError - True for an error (red), false for info (green).
         */
        function showMessage(message, isError, targetMessageBox = messageBox) {
            targetMessageBox.textContent = message;
            targetMessageBox.classList.remove('error', 'info');
            targetMessageBox.classList.add(isError ? 'error' : 'info', 'show');
        }

        /**
         * Hides the message box.
         */
        function hideMessage(targetMessageBox = messageBox) {
            targetMessageBox.classList.remove('show');
        }

        /**
         * Retries a fetch request with exponential backoff.
         * @param {string} url - The URL to fetch.
         * @param {object} options - The fetch options.
         * @param {number} maxRetries - Maximum number of retries.
         * @returns {Promise<Response>} The response from the successful fetch.
         */
        async function fetchWithRetry(url, options, maxRetries = 5) {
            let retries = 0;
            let delay = 1000; // Initial delay of 1 second
            while (retries < maxRetries) {
                try {
                    const response = await fetch(url, options);
                    // If response is successful or a non-retryable client error, return it
                    if (response.ok || response.status < 500 && response.status !== 429) {
                        return response;
                    }
                    // For server errors or rate limits, retry
                    console.warn(`Fetch failed with status ${response.status}. Retrying in ${delay / 1000}s...`);
                    await new Promise(resolve => setTimeout(resolve, delay));
                    delay *= 2;
                    retries++;
                } catch (error) {
                    console.error(`Fetch failed with error: ${error.message}. Retrying in ${delay / 1000}s...`);
                    await new Promise(resolve => setTimeout(resolve, delay));
                    delay *= 2;
                    retries++;
                }
            }
            throw new Error(`Failed to fetch from ${url} after ${maxRetries} retries.`);
        }

        /**
         * Converts a Base64 string to an ArrayBuffer.
         * @param {string} base64 - The Base64 string.
         * @returns {ArrayBuffer}
         */
        function base64ToArrayBuffer(base64) {
            const binaryString = window.atob(base64);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }

        /**
         * Converts PCM audio data to a WAV Blob.
         * @param {Int16Array} pcmData - The PCM data.
         * @param {number} sampleRate - The sample rate.
         * @returns {Blob}
         */
        function pcmToWav(pcmData, sampleRate) {
            const dataLength = pcmData.length * 2;
            const buffer = new ArrayBuffer(44 + dataLength);
            const view = new DataView(buffer);

            const writeString = (view, offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };

            // RIFF identifier
            writeString(view, 0, 'RIFF');
            // file length
            view.setUint32(4, 36 + dataLength, true);
            // RIFF type
            writeString(view, 8, 'WAVE');
            // format chunk identifier
            writeString(view, 12, 'fmt ');
            // format chunk length
            view.setUint32(16, 16, true);
            // sample format (raw)
            view.setUint16(20, 1, true);
            // channel count
            view.setUint16(22, 1, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true);
            view.setUint16(32, 2, true);
            // bits per sample
            view.setUint16(34, 16, true);
            // data chunk identifier
            writeString(view, 36, 'data');
            // data chunk length
            view.setUint32(40, dataLength, true);

            let offset = 44;
            for (let i = 0; i < pcmData.length; i++, offset += 2) {
                view.setInt16(offset, pcmData[i], true);
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }


        /**
         * Plays audio from a text prompt using the Gemini TTS API.
         * This function now only handles the core API call and audio playback.
         * @param {string} text - The text to speak.
         */
        async function speakText(text) {
            const selectedVoice = voiceSelect.value;

            const payload = {
                contents: [{
                    parts: [{ text: text }]
                }],
                generationConfig: {
                    responseModalities: ["AUDIO"],
                    speechConfig: {
                        voiceConfig: {
                            prebuiltVoiceConfig: { voiceName: selectedVoice }
                        }
                    }
                },
                model: "gemini-2.5-flash-preview-tts"
            };

            // The API key is now an empty string, which the platform will automatically populate.
            const apiKey = "";
            const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent?key=${apiKey}`;
            
            const response = await fetchWithRetry(apiUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
            });
            
            if (!response.ok) {
                throw new Error(`TTS API error: ${response.status} ${response.statusText}`);
            }

            const result = await response.json();
            
            const candidate = result?.candidates?.[0];
            if (!candidate || candidate.finishReason === 'OTHER') {
                console.error("TTS API response was missing expected structure or failed with finishReason 'OTHER'.", result);
                throw new Error("The TTS API failed to generate audio for this response. Please try again.");
            }

            const part = candidate.content?.parts?.[0];
            const audioData = part?.inlineData?.data;
            const mimeType = part?.inlineData?.mimeType;

            if (audioData && mimeType && mimeType.startsWith("audio/")) {
                const sampleRateMatch = mimeType.match(/rate=(\d+)/);
                const sampleRate = sampleRateMatch ? parseInt(sampleRateMatch[1], 10) : 16000;
                const pcmData = base64ToArrayBuffer(audioData);
                const pcm16 = new Int16Array(pcmData);
                const wavBlob = pcmToWav(pcm16, sampleRate);
                const audioUrl = URL.createObjectURL(wavBlob);

                return audioUrl;
            } else {
                console.error("TTS API response had invalid audio data or mime type.", result);
                throw new Error("Invalid or missing audio data in TTS response.");
            }
        }

        /**
         * Wraps speakText with a retry mechanism to handle transient API failures.
         * @param {string} text - The text to speak.
         * @param {function} onAudioEndCallback - A callback function to run when the audio finishes.
         */
        async function speakTextWithRetry(text, onAudioEndCallback = null) {
            setState('SPEAKING');

            const maxRetries = 3;
            let retries = 0;
            let audioUrl = null;
            let lastError = null;

            while (retries < maxRetries) {
                try {
                    audioUrl = await speakText(text);
                    if (audioUrl) {
                        break;
                    }
                } catch (error) {
                    lastError = error;
                    // Added a console.log to show the specific text that failed.
                    console.warn(`TTS generation for text "${text}" failed. Retrying... (Attempt ${retries + 1}/${maxRetries})`);
                    retries++;
                    await new Promise(resolve => setTimeout(resolve, 2000));
                }
            }
            
            if (audioUrl) {
                currentAudio = new Audio(audioUrl);
                currentAudio.onended = () => {
                    if (onAudioEndCallback) {
                        onAudioEndCallback();
                    }
                    currentAudio = null;
                    URL.revokeObjectURL(audioUrl);
                };
                currentAudio.play();
            } else {
                console.error('Error during TTS:', lastError);
                showMessage(`Assistant failed to speak the response. The response was: "${text}"`, true);
                if (onAudioEndCallback) {
                    onAudioEndCallback();
                }
                // Re-throw the error so that the caller can handle it gracefully.
                throw lastError || new Error("Failed to speak text after multiple retries.");
            }
        }
        
        /**
         * Sends a user prompt to the Gemini LLM API and handles the response.
         * @param {string} prompt - The user's text prompt.
         * @param {string} langCode - The language for the generation prompt.
         */
        async function getLLMResponse(prompt, langCode) {
            setState('THINKING');

            // Speak "I'm thinking" first, then proceed with the LLM call.
            try {
                await speakTextWithRetry("I'm thinking.", async () => {
                    // This callback runs after "I'm thinking" has finished playing.
                    
                    // The text that will be used for the response
                    let responseText;

                    const lowerPrompt = prompt.toLowerCase();
                    // Define keywords for creator-related queries
                    const creatorKeywords = ["who created you", "who made you", "your creator", "who built you"];
                    // Check if the user's prompt is about the creator
                    const isCreatorQuery = creatorKeywords.some(keyword => lowerPrompt.includes(keyword));

                    if (isCreatorQuery) {
                        // Hardcoded response for creator queries
                        responseText = "I am created by Raji Technology Innovator Amer";
                    } else {
                        // Normal LLM API call
                        const apiKey = "";
                        const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${apiKey}`;

                        const fullPrompt = `You are a helpful and conversational AI assistant. Respond in a complete sentence in the language of the user's query, which is ${langCode}. The user said: "${prompt}"`;

                        const payload = {
                            contents: [{ role: "user", parts: [{ text: fullPrompt }] }]
                        };

                        try {
                            const response = await fetchWithRetry(apiUrl, {
                                method: 'POST',
                                headers: { 'Content-Type': 'application/json' },
                                body: JSON.stringify(payload)
                            });

                            if (!response.ok) {
                                throw new Error(`LLM API error: ${response.status} ${response.statusText}`);
                            }

                            const result = await response.json();
                            responseText = result?.candidates?.[0]?.content?.parts?.[0]?.text;
                        } catch (error) {
                            console.error('Error during LLM call:', error);
                            showMessage(`Assistant failed to generate a response: ${error.message}`, true);
                            setState('HOTWORD_LISTENING');
                            startHotwordListening();
                            return; // Stop execution here
                        }
                    }

                    if (responseText && responseText.trim().length > 0) {
                        assistantOutputDiv.textContent = `Assistant: "${responseText}"`;
                        
                        try {
                            await speakTextWithRetry(responseText, () => {
                                setState('HOTWORD_LISTENING');
                                startHotwordListening();
                            });
                        } catch (ttsError) {
                            console.error('TTS failed, but the text was generated successfully. Displaying text instead.', ttsError);
                            assistantOutputDiv.textContent = `Assistant (text only): "${responseText}"`;
                            showMessage(`TTS failed to generate audio for this response. Displaying text instead.`, true);
                            setState('HOTWORD_LISTENING');
                            startHotwordListening();
                        }
                    } else {
                        console.warn("LLM returned a very short or empty response, skipping TTS.");
                        assistantOutputDiv.textContent = `Assistant: "${responseText}"`;
                        showMessage("Assistant provided a short response. Hotword listening will resume.", false);
                        setState('HOTWORD_LISTENING');
                        startHotwordListening();
                    }
                });
            } catch (error) {
                console.error('Error speaking "I\'m thinking.":', error);
                showMessage('Assistant failed to say "I\'m thinking."', true);
                setState('HOTWORD_LISTENING');
                startHotwordListening();
            }
        }

        /**
         * Initializes and starts the main speech recognition instance for user queries.
         */
        function startListening() {
            // Ensure any existing listeners are stopped to prevent conflicts
            if (hotwordRecognition) hotwordRecognition.stop();
            if (recognition) recognition.stop();
            hideMessage();
            setState('MAIN_LISTENING');

            recognition = new SpeechRecognition();
            recognition.continuous = false; // Capture a single phrase
            recognition.interimResults = false; // Only return final results
            recognition.lang = languageSelect.value;
            recognition.maxAlternatives = 10; // Increased alternatives for better distant listening

            recognition.onstart = () => {
                userInputDiv.textContent = '';
                assistantOutputDiv.textContent = '';
                console.log('Main recognition started.');
            };

            recognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                userInputDiv.textContent = `You: "${transcript}"`;
                console.log(`Transcript: "${transcript}"`);
                
                if (transcript.trim() === "") {
                    showMessage("No speech detected. Hotword listening will resume.", false);
                    startHotwordListening();
                    return;
                }

                getLLMResponse(transcript, languageSelect.value);
            };

            recognition.onerror = (event) => {
                if (event.error === 'not-allowed') {
                    showMessage("Microphone access denied. Please allow microphone access and try again.", true);
                } else if (event.error === 'no-speech') {
                    showMessage("No speech detected. Hotword listening will resume.", false);
                } else {
                    showMessage(`Speech recognition error: ${event.error}`, true);
                }
                console.error('Main recognition error:', event.error);
                startHotwordListening();
            };

            recognition.onend = () => {
                console.log('Main recognition ended.');
                // If not in a thinking or speaking state, revert to hotword listening.
                if (state !== 'THINKING' && state !== 'SPEAKING') {
                    startHotwordListening();
                }
            };

            try {
                recognition.start();
            } catch (e) {
                console.error("Error starting main recognition:", e);
                showMessage("Could not start microphone. Please ensure it's available and permissions are granted.", true);
                startHotwordListening();
            }
        }

        /**
         * Initializes and starts the hotword recognition instance.
         */
        function startHotwordListening() {
            // Stop any other listeners
            if (recognition) recognition.stop();
            if (hotwordRecognition) hotwordRecognition.stop();

            setState('HOTWORD_LISTENING');
            console.log("Starting hotword recognition session.");
            
            hotwordRecognition = new SpeechRecognition();
            hotwordRecognition.continuous = true; // Listen continuously
            hotwordRecognition.interimResults = false;
            hotwordRecognition.lang = languageSelect.value;
            hotwordRecognition.maxAlternatives = 10; // Increased alternatives for better distant listening

            hotwordRecognition.onresult = (event) => {
                const transcript = event.results[event.results.length - 1][0].transcript.toLowerCase();
                console.log(`Hotword Listener Transcript: "${transcript}"`);
                if (transcript.includes('jarvis')) {
                    console.log('Hotword "Jarvis" detected!');
                    hotwordRecognition.stop(); // Stop the continuous listener
                    speakTextWithRetry("What can I help you with?", () => {
                        // After assistant speaks, start the main listening session
                        startListening();
                    });
                }
            };
            
            hotwordRecognition.onerror = (event) => {
                if (event.error === 'not-allowed') {
                    showMessage("Microphone access denied. Please allow microphone access in your browser settings and try again.", true);
                    setState('IDLE');
                } else {
                    console.warn(`Hotword recognition error: ${event.error}. Restarting...`);
                    hotwordRecognition.stop(); // Explicitly stop to trigger onend
                }
            };
            
            hotwordRecognition.onend = () => {
                console.log('Hotword recognition ended.');
                // Always restart on an unexpected end, unless a different state is active.
                if (state === 'HOTWORD_LISTENING') {
                    console.log("Restarting hotword listener due to onend event.");
                    setTimeout(startHotwordListening, 1000);
                }
            };

            try {
                hotwordRecognition.start();
            } catch (e) {
                console.error("Error starting hotword recognition:", e);
                showMessage("Could not start microphone. Please ensure it's available and permissions are granted.", true);
                // In case of an error, we should stop and not try to restart immediately
                // to avoid an infinite loop of failures.
                setState('LOADING');
            }
        }

        /**
         * Initializes the voice select dropdown with available prebuilt voices.
         */
        function populateVoices() {
            voiceSelect.innerHTML = '';
            prebuiltVoices.forEach(voice => {
                const option = document.createElement('option');
                option.value = voice.name;
                option.textContent = voice.label;
                voiceSelect.appendChild(option);
            });
            // Set a default voice if needed
            voiceSelect.value = 'Kore'; 
        }

        // Initialize the app on window load
        window.onload = () => {
            populateVoices();
            // Start the hotword listener immediately on load.
            startHotwordListening();
        };
    </script>
</body>
</html>
